{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XXpLAcHIE94_"
      },
      "outputs": [],
      "source": [
        "import requests as req\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from datetime import datetime\n",
        "import csv\n",
        "import re\n",
        "import string\n",
        "\n",
        "hades = { 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sWaHGfyFHAUk"
      },
      "outputs": [],
      "source": [
        "def clear_content(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"#[A-Za-z0-9_]+\", \"\", text)\n",
        "    text = re.sub(r'http\\S+', '', text)  # remove links\n",
        "    text = re.sub(r'[0-9]+', '', text)  # remove numbers\n",
        "    text = text.replace('\\n', ' ')  # remove new lines\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # remove punctuation\n",
        "    text = text.strip()  # trim spaces\n",
        "    text = text.replace(\"¬©\", '')  # remove special symbols\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ai2jNsa4vmGY"
      },
      "outputs": [],
      "source": [
        "def clean_additional_content(content):\n",
        "    # Remove specific phrases\n",
        "    content = content.replace(\"jakarta cnbc indonesia\", \"\")\n",
        "    content = content.replace(\"jakarta cnbcÂ indonesiaÂ\", \"\")\n",
        "    content = content.replace(\"advertisement\", \"\")\n",
        "    content = content.replace(\"scroll to resume content\", \"\")\n",
        "    content = content.replace(\"Â\", \"\")\n",
        "\n",
        "    # Remove extra spaces\n",
        "    content = ' '.join(content.split())\n",
        "\n",
        "    # Remove empty lines\n",
        "    content = '\\n'.join(line.strip() for line in content.splitlines() if line.strip())\n",
        "\n",
        "    return content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Jws-Xjk9Zynt"
      },
      "outputs": [],
      "source": [
        "def scrape_cnbc(hal):\n",
        "    global hades\n",
        "    a = 1\n",
        "    csv_file = 'cnbc_result.csv'\n",
        "\n",
        "    # Write the header to the CSV file\n",
        "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file, delimiter=',')\n",
        "        writer.writerow(['Headline', 'Date', 'Link', 'Content'])\n",
        "\n",
        "    for page in range(1, hal+1):\n",
        "        url_cnbc = f\"https://www.cnbcindonesia.com/search?query=bea%20cukai&p={page}&kanal=&tipe=artikel&date=\"\n",
        "        ge = req.get(url_cnbc, headers=hades).text\n",
        "        sop = bs(ge, 'lxml')\n",
        "        ul = sop.find('ul', class_='list media_rows middle thumb terbaru gtm_indeks_feed')\n",
        "        if ul is None:\n",
        "            continue\n",
        "        li = ul.find_all('li')\n",
        "        for x in li:\n",
        "            link_art = x.find('article').find('a')['href']\n",
        "            ge_ = req.get(link_art, headers=hades).text\n",
        "            sop_ = bs(ge_, 'lxml')\n",
        "            art_div = sop_.find('div', class_='lm_content mt10')\n",
        "            if art_div is None:\n",
        "                continue\n",
        "            art = art_div.find('article')\n",
        "            if art is None:\n",
        "                continue\n",
        "            headline = art.find('h1').text\n",
        "            date_text = art.find('div', class_='date').text\n",
        "            date = datetime.strptime(date_text, '%d %B %Y %H:%M')\n",
        "            warp = art.find_all('div', class_='detail_text')\n",
        "            for w in warp:\n",
        "                paragraphs = w.find_all('p')\n",
        "                content = ''.join(p.text for p in paragraphs)\n",
        "                content = clear_content(content)  # clean the content\n",
        "                content = clean_additional_content(content)  # clean additional content\n",
        "                print(f'done[{a}] > {headline[0:10]}')\n",
        "                a += 1\n",
        "                with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
        "                    writer = csv.writer(file, delimiter=',')\n",
        "                    writer.writerow([headline, date, link_art, content])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
